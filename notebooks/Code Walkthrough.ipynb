{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload modules when they change.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying the path so we can import from src directory.\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import copy\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from src.example_graphs import simple_undirected_graph, simple_directed_graph\n",
    "from src.UndirectedGraph import UndirectedGraph\n",
    "from src.DirectedGraph import DirectedGraph\n",
    "from src.DataLoader import DataLoader\n",
    "from src.GraphCreator import GraphCreator\n",
    "\n",
    "from src.io_helpers import pickle_obj, load_pickled_obj\n",
    "from src.networkx_helpers import combine_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': ['B'], 'B': ['A', 'C', 'D'], 'C': ['B', 'D'], 'D': ['B', 'C'], 'E': []}\n"
     ]
    }
   ],
   "source": [
    "# Create simple undirected graph.\n",
    "g = simple_undirected_graph()\n",
    "g_class = UndirectedGraph(g)\n",
    "print(g_class.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple directed graph.\n",
    "g_directed = simple_directed_graph()\n",
    "g_directed_class = DirectedGraph(g_directed)\n",
    "\n",
    "# Test DirectedGraph class functionality.\n",
    "g_directed_class.add_node('F', ('A', 'C'))\n",
    "g_directed_class.add_edge('C', 'F')\n",
    "g_directed_class.remove_edge('F', 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': ['B'], 'B': ['C', 'D'], 'C': ['D', 'F'], 'D': [], 'E': [], 'F': ['A']}\n"
     ]
    }
   ],
   "source": [
    "print(g_directed_class.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Reddit Hyperlink Data into Custom Graph Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of how you could read the data and pickle it.\n",
    "\n",
    "```python\n",
    "# Read in some lines of Reddit Hyperlink data.\n",
    "start_time = time.time()\n",
    "data_loader = DataLoader(num_lines=1000, cols_to_load=['SOURCE_SUBREDDIT', 'TARGET_SUBREDDIT'])\n",
    "lines = data_loader.load()\n",
    "\n",
    "graph_creator = GraphCreator()\n",
    "reddit_body_hyperlink_graph = graph_creator.create_graph(lines)\n",
    "graph_creator.pickle_graph(reddit_body_hyperlink_graph, \"data_pickle/reddit_body_1000.pickle\")\n",
    "end_time = time.time()\n",
    "print(\"Time to load and pickle 1000 rows: {}\".format(end_time - start_time))\n",
    "\n",
    "# Create full pickle file of the non-multigraph Reddit Hyperlink body data.\n",
    "start_time = time.time()\n",
    "data_loader = DataLoader(full_file=True, cols_to_load=['SOURCE_SUBREDDIT', 'TARGET_SUBREDDIT'])\n",
    "lines = data_loader.load()\n",
    "\n",
    "graph_creator = GraphCreator()\n",
    "reddit_body_hyperlink_graph = graph_creator.create_graph(lines)\n",
    "graph_creator.pickle_graph(reddit_body_hyperlink_graph, \"data_pickle/reddit_body_full_non_multigraph.pickle\")\n",
    "end_time = time.time()\n",
    "print(\"Time to load and pickle full file (non-multigraph): {}\".format(end_time - start_time))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Data in Pandas\n",
    "Before analyzing the graph data via the graph structure, let's just look at some basics in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def value_count_overlap(counts1, counts2, n, return_list=False):\n",
    "    overlap = set(counts1.head(n).index).intersection(set(counts2.head(n).index))\n",
    "    if return_list:\n",
    "        return len(overlap), overlap\n",
    "    else:\n",
    "        return len(overlap)\n",
    "\n",
    "df_body = pd.read_csv(\"data/soc-redditHyperlinks-body.tsv\", delimiter='\\t')\n",
    "df_title = pd.read_csv(\"data/soc-redditHyperlinks-title.tsv\", delimiter='\\t')\n",
    "\n",
    "# Look at df_body\n",
    "print(df_body.info())\n",
    "print(df_body.describe())\n",
    "\n",
    "# Look at df_body\n",
    "print(df_title.info())\n",
    "print(df_title.describe())\n",
    "\n",
    "# Get post ids from each file.\n",
    "title_post_ids = set(df_title['POST_ID'])\n",
    "body_post_ids = set(df_body['POST_ID'])\n",
    "\n",
    "# Are there any post ids in common? (Nope. We get 0 here.)\n",
    "inter = title_post_ids.intersection(body_post_ids)\n",
    "print(len(inter))\n",
    "\n",
    "# Let's look at the top subreddits by where they're posted.\n",
    "body_source_counts = df_body[\"SOURCE_SUBREDDIT\"].value_counts()\n",
    "title_source_counts = df_title[\"SOURCE_SUBREDDIT\"].value_counts()\n",
    "\n",
    "# Are there commonalities between the two datasets for SOURCE?\n",
    "print(set(body_source_counts.head(10).index).intersection(set(title_source_counts.head(10).index)))\n",
    "print(set(body_source_counts.head(20).index).intersection(set(title_source_counts.head(20).index)))\n",
    "print(set(body_source_counts.head(50).index).intersection(set(title_source_counts.head(50).index)))\n",
    "\n",
    "# Let's look at the top subreddits by who they're referencing / posting about.\n",
    "body_target_counts = df_body[\"TARGET_SUBREDDIT\"].value_counts()\n",
    "title_target_counts = df_title[\"TARGET_SUBREDDIT\"].value_counts()\n",
    "\n",
    "# Let's look at the top 10 for each file.\n",
    "print(body_target_counts.head(10))\n",
    "print(title_target_counts.head(10))\n",
    "\n",
    "# Are there commonalities between the two datasets for TARGET?\n",
    "print(set(body_target_counts.head(10).index).intersection(set(title_target_counts.head(10).index)))\n",
    "print(set(body_target_counts.head(20).index).intersection(set(title_target_counts.head(20).index)))\n",
    "print(set(body_target_counts.head(50).index).intersection(set(title_target_counts.head(50).index)))\n",
    "\n",
    "# Now let's look at what the top SOURCE and TARGET are for the combined df.\n",
    "df_concat = pd.concat([df_body, df_title])\n",
    "concat_source_counts = df_concat[\"SOURCE_SUBREDDIT\"].value_counts()\n",
    "concat_target_counts = df_concat[\"TARGET_SUBREDDIT\"].value_counts()\n",
    "\n",
    "# Let's look at the top 10 for SOURCE and TARGET for the concat df.\n",
    "print(concat_source_counts.head(10))\n",
    "print(concat_target_counts.head(10))\n",
    "\n",
    "# Now let's look at the counts of the overlap with the body and title datasets\n",
    "print(value_count_overlap(concat_source_counts, body_source_counts, 10))\n",
    "print(value_count_overlap(concat_source_counts, title_source_counts, 10))\n",
    "print(value_count_overlap(concat_target_counts, body_target_counts, 10))\n",
    "print(value_count_overlap(concat_target_counts, title_target_counts, 10))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
